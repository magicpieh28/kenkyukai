# D2 콘도상
- 타클 내서 거절 당했지만 선행연구 더 참조하면 다시 검토해줄게.
- 리뷰어한테 무슨 얘기 듣는지.
### Submit to TACL
- TACL에 내면 언제든지 원하는 때에 ACL에 낼 수 있게 됨.
- 최대 10페이지 안에 내 주세요. 하지만 에이씨엘 이엠엔엘피 나클 3개 연속으로 낼 것인지 타클에만 낼 것인지 정해야 함.
- 지금 연구를 하고 있어도 3월까지 시간이 부족하다 싶으면 여름 직전까지 타클에 낼 수 있다는 정보.
### 새 연구
- 멀티 도큐멘트 요약
  - 9~10월에는 옛날 논문을 읽으면서 정리
  - 인터넷에 올라와 있는 의논, 리뷰, 코멘트 등을 요약
  - 사전 실험?: 문장의 중요도를 알아내서 문장을 전처리해서 요약
  - 전처리로 쓸데없는 데이터가 없어지니 정확도가 높아질 줄 알았더니 낮아졌다고. 뉴스 30만개를 훈련에 썼는데 그래도 안됐다고.
  - 결론: 재현 불가(소스에서 타겟을 알아내는 것이 아니라 타겟에서 타겟을 알아냄? 뭔소리야)
  - 이게 잘 됐으면 세미 슈퍼바이즈드나 트랜스퍼 할라 했는데 잘 안나와서 못함
 - 데이터
   - 코스트가 높아서 제대로 된 데이터 구하기가 힘듦
   - 모든 텍스트 데이터를 고려할 수는 없음
   - 시퀀스 넣는 것은 포기하고 모든 데이터를 어떻게든 데이터화 해서 넣어서 데이터와 타겟의 관계만 보면 되니까 병렬 코퍼스 필요없구나
   - 문장 엠베딩, 문서 엠베딩을 적당히 어텐션 걸면 문장 구조 정보가 없ㅂ어져버림
   - 그렇다고 전체를 넣을 수도 없으니 그래프를 이용해서 복수의 문서를 취급함
   - 데이터셋 직접 만듦
- 그래프 입력
  - 한 문서를 그래프로 변환해서 넣으면 문장 구조도 잘 보존되지 않을까
  - 그래프에서 타겟을 생성함
  - 그래프라면 인간이 잘 변환할 수 있으니까?
  - 사전 id - 인접 행렬
  - 인접 행렬은 바이그램 반전시킨 것
  - 출력은 두개, 벡터화 시킨 것.
  - 샘플 하나에 대해 그래프 엠베딩(인접 행렬을 인수로) 이걸 인코더에 넣음
  - 노드 엠베딩과 엘에스티엠 하면서 아웃풋 얻음
  - 계산도 빨라짐
- 주변 연구
  - The WebNLG Challenge Generating Text from RDF Data
  - Structured Sequence Modeling with Graph Convolutional Recurrent Networks
  - Semi-Supervised Classification with Graph Convolutional Networks
  - 한 문장을 생성하면 되기 때문에 데이터도 적어도 되었지만
  - 이번에는 논문의 앱스트렉트 정도 길이 되는 것을 생성하고 싶음 (최대 200 문자)
- 연구 (GraphNN)
  - 한 섹션을 텍스트로 취급함
  - 데이터셋 전체 19만
  - 총 단어수 4300만
  - 타겟 2500 정도
  - 노드는 5분의 일 혹은 7분의 일 까지 줄일 수 있으므로 데이터 축소가 가능함
  - 노드 워드 토큰 (데이터 포맷)
  - 문서 앞에서 부터 나타나는 단어가 앞 번호
  - bi-lstm 훈련 최대 4초 = 7배 빨라짐
- 결과
  - 길이는 짧지만 내용은 비슷하다고 생각됨
  - 데잍터가 많아도 그래프가 있다면 얼마든지 줄일 수 있구나
  - 실패: 시퀀스 레벨, 단어 레벨로 생성되어버림. 목적, 수법, 결과에 대한 라벨이 붙어있는 데이터로 학습 했으니까 생성도 비슷한 형식이 나옴. = 문장 구조 자체도 생성 가능하구나
- 앞으로 해야할 일
  - 비슷한 얘기가 이미 나와 있으니 멀티 태스크 러닝과 트랜스퍼 러닝으로도 연구를 진행해보자