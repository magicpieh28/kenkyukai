# 시바하라상
- 확률모델을 이용한 방법과 뉴럴넷을 이용한 방법: 뉴럴넷이 확률모델보다 시간 오래 걸림
- 논문 분류와 계층도를 만들고 싶음?
- 키 문구 추출, 분류, 데이터셋 계층 구조 ...
  - 메소드에 사용된 수법 인용 문맥 속에서 키워드 추출
  - 수법 키워드 클러스터링
  - 기사 맵핑
  - 계층 가시화에 기초한 키워드 그룹 작성
- 목표: 수법에 대한 키워드를 추출하는 것으로 계층구조를 빌드
- TACL2018: Measuring the evolution of a scientific field through citation frames
- 유명한 논문만을 고려해도 될 것 같은 = 인용 횟수가 높은 논문은 생략해서 적는 경우가 많으므로 생략자도 처리해야 함
- ACL2018 demo paper: A Web-scale system for scientific knowledge explanation
### 교수님 피드백
- 너가 뭘 하고 싶은지에 대한 얘기는 별로 없네
- 예전부터 많이 하던 방식은: seed를 준비해서 패턴을 찾음
- 꼭 특정 단어가 수법을 가리키고 있는 것은 아니고 문맥에 따라 달라지니까 실제로 수법을 묘사하는 키워드로 사용되었는지가 중요
- 맹라성은 있는 얘기인지: 아이자와씨의 계층도처럼 줄임말을 주로 가져옴
- 좀 더 구체적인 이야기를 오늘은 들을 수 있나 생각했는데
- 태스크별로 라벨 붙여놓고 클러스터링을 할 것인지
- 솔직히 이 논문이 어떤 수법을 가장 중요하게 생각하는지를 알고 싶다면 계층구조의 어디에 포함되어있는지는 알 필요없고 키워드만 가져오면 되지 않나: 검색할 때 필요하게 ...
- 계층구조를 나무구조랑 같다고 생각하면 되나?: 그건 아니고.. 클러스에 속하는 특성을 알면 계층구조는 자동으로 생성될텐데
- 계층구조는 원래 저렇게 거꾸로 된 것 같이 생겼어?: 그건 제가 잘 몰라서 그렇게 된 것이지 ... ㅎㅎㅎ

# 혼다상
- visual entailment
- 자연언어추론(NLI)에 있어서 사진은 별로 좋은 역할을 하고 있지 않음
- 석사논문: grounded textual entailment
  - 단어레벨로 사진에 어텐션(소프트맥스로 정규화하니까 null attention이라는 어텐션)
  - 일부러 더미를 만들어서 어떤 사진에도 대응하지 않으면 dummy에 어텐션을 많이 걸어서 전체를 소프트맥스로 정규화
  - dummy에 어텐션 높은 것은 사진 말고 단어/문장에만 대응할 수 있다는 것이므로 단어 엠베딩을 특징량으로
- 극복대상: 인공적인 데이터셋, 하나의 분야에만 쏠려 있는 데이터셋
  - SNLI-VE, VOA-20?
- 사진과 가설(hypothesis: 사진 캡션?)에 어텐션 걸어서 사진 정보(사진 엠베딩)를 biLSTM(공유)과 MLP(맥스 풀링)에 통과하는 모델과
  - image comparison model
  - "biLSTMをかますのは"
- 사진과 가설에 어텐션 걸어서 가설(단어 엠베딩) 정보(단어엠베딩과 사진 엠베딩)를 biLSTM(공유)과 MLP에 통과하는 모델로 실험
  - sentence comparison model
  - dummy 어텐션만 사용
  - MLP에 사진 정보만 넣으면 스코어가 낮아짐
- 다른 도메인에 어텐션 걸어도 제대로 ??가 안되니까 일부러 이번엔 이렇게 설정
  - 가장 정확도가 높은 선행연구보다 2점씩 높아짐
- 선행연구 중 정도가 높았던 것은 EVE: NIPS workshop
  - CNN 사용
  - "プレトレインのモデルかなんかに食べさせて"
- null 어텐션은 추상적인 단어 부분의 숫자가 높아짐
  - dummy를 없애면 선행연구랑 비슷한 스코어
  - sentence comparison model에서 dummy를 없애려는 생각은 해보지 않았지만 어텐션 가중치에서 dummy를 없애버리면 0 혹은 1만 나오는
### 교수님 피드백
- dummy가 뭘 하는진 잘 모르겠지만
- dummy의 가중치가 높다는 것은 사진에 정보가 없다는 얘기지만 dummy 가중치가 낮다는 것은 사진에 정보가 충분하다는 얘기잖아
- dummy에 유루이한 어텐션을 적용하는 정도가 아니면 ??가 안되니까
###
어떻게 이렇게 계산 구조랑 알고리즘 원리랑 잘 알지?
내가 제일 안 돌아가는 부분의 머리가 혼다상은 존나 잘 돌아감
수학하던 사람도 아닌데...???? 어떻게???? 인턴 갔다오면 이렇게 잘 알게 되는건가?

# 와다상
- 취미얘기할래요
  - Takashionary: 페이스북 유저가 많이 늘었습니다!
  - 유명한 기업 계정보다 팔로워 수가 쩔어요!
  - 위키피디아 위키셔너리 다음으로 나와요!
  - NHK보다 검색결과가 위에 나와요!
- 비지도 크로스링구얼 단어 엠베딩 비교
  - 프리트레인 엠베딩 맵핑: 1 언어에서 2 언어로 맵핑할 때의 선형 transformation(W)을 비지도로 하는게 유행중
  - vocabulary를 분포의 분포로 나타내면 거리 최소화가 잘 될 것?: Level-2 Kernel
  - NAACL 2019: cross-lingual alignment of contextual word embedding with applications to zero-shot dependency parsing
  - 단어를 문맥 벡터로 나타냈을 때 잘 되는지 실험해 본 결과 아직까지는 나쁜 결과밖에 안 나옴
  - 디코딩 단어의 순서대로 가져오기 때문에 majority를 극복하지 않으면
- 문법 에러 교정(誤り訂正, GEC)
  - end to end가 가장 유명
  - NMT와 다른 점은 ...
  - GEC는 일반화된 규칙을 학습할 필요가 있음
  
전체적으로 전혀 다른 분야 얘기라서 집중이 안됨..ㅋ
  